---
layout: post
title: "CVX-1: A review on convex optimization"
date: 2020-04-14 22:37:59 +0800
tags: [optimization, convex optimization]
subtitle: "1. The Lagrangian and Dual"
author: "brentian"
---

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Chuwen Zhang" />
  <script src="https://cdn.jsdelivr.net/npm/mermaid@8.4.0/dist/mermaid.min.js"></script>
  <script>mermaid.initialize({ startOnLoad: true });</script>
</head>

<body>
  <nav id="TOC" role="doc-toc">
    <ul>
      <li><a href="#definition-of-dual">Definition of Dual</a>
        <ul>
          <li><a href="#definitions">Definitions</a></li>
        </ul>
      </li>
      <li><a href="#weak-strong-duality">Weak &amp; Strong Duality</a>
        <ul>
          <li><a href="#geometric-interpretation">Geometric interpretation</a></li>
          <li><a href="#slaters-condition">Slater’s condition</a></li>
          <li><a href="#examples">Examples</a></li>
        </ul>
      </li>
      <li><a href="#reference">Reference</a></li>
    </ul>
  </nav>
  <h2 id="definition-of-dual">Definition of Dual</h2>
  <p>Standard form of optimization problems: <span class="math display">\min_x f(x) </span></p>
  <p>s.t.<br />
    <span class="math display">h_i(x) = 0, i =1,2,...</span> <span class="math display">f_i(x) \le 0, i=1,2,...</span>
  </p>
  <h3 id="definitions">Definitions</h3>
  <ul>
    <li>{the lagrangian} The lagrangian function <span class="math inline">L</span>: [ <span
        class="math inline">F,H</span> are vector-valued functions of <span class="math inline">x:\{f_j,h_j\}_j</span> ]
    </li>
  </ul>
  <p><span class="math display">L(x,\lambda,\mu) = f + \lambda^TF+\nu^TH</span></p>
  <ul>
    <li>{lagrange} Dual function <span class="math inline">\displaystyle g(\lambda, \nu) = \inf_x L</span>
      <ul>
        <li>is a function of <span class="math inline">\lambda, \nu</span>, which is piecewise linear (point-wise
          minimum) and thus <strong>concave</strong></li>
        <li>this infers that maximizing <span class="math inline">g</span> is a convex optimization problem</li>
      </ul>
    </li>
    <li>notice: w.t. <span class="math inline">\lambda\ge 0</span></li>
  </ul>
  <p><span class="math display">g(\lambda, \nu) \le L \le f, \forall x \text{ feasible}</span> <span
      class="math display">\Rightarrow \sup_{\lambda \ge 0} g \le \inf_{H=0, F\le 0} f</span></p>
  <ul>
    <li><span class="math inline">g</span> is the minimum over the set of <span class="math inline">x</span>, thus
      concave [ like a piecewise affine function with possibly infinite pieces … ]</li>
  </ul>
  <h2 id="weak-strong-duality">Weak &amp; Strong Duality</h2>
  <ul>
    <li>
      <p>Weak</p>
      <p>from previous definition we have the weak duality:</p>
      <p><span class="math display">g(\lambda,\nu) \le \inf_x L \le L(x), \quad \forall x\;\text{feasible}</span></p>
    </li>
    <li>
      <p>Weak properties</p>
      <ul>
        <li>for min. primal problems, dual (max.) provides a lower bound, this is useful for terminating an optimization
          algorithm (by checking the <strong>gap</strong>)</li>
        <li>unbounded (unbd) and infeasible verification, if dual is unbounded, the primal shall be infeasible.</li>
      </ul>
    </li>
    <li>
      <p>Strong</p>
      <p>the weak is to be strong if some conditions hold. common qualifications for <strong>SD</strong>:</p>
      <ul>
        <li>primal problem is a LP</li>
        <li>if the problem is convex and Slater’s condition holds.</li>
        <li>also, <strong>SD</strong> can hold for nonconvex problems.
          <ul>
            <li>e.g. single constraint quadratic problem</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
  <h3 id="geometric-interpretation">Geometric interpretation</h3>
  <p>define: <span class="math display">\mathcal{G} = \big\{(u,v,t)|u=F,v=H,t=f, x\in D\big\}</span></p>
  <p>be value set of the problem, the optimal value is then:</p>
  <p><span class="math display">f^\star = \inf_t \{(u,v,t) \in \mathcal{G}, \;u\le 0, v = 0\}</span></p>
  <p>also define extended set: (or epigraph)</p>
  <p><span class="math display">\mathcal{A} = \big\{(u,v,t)|u\ge F,v=H,t\ge f, x\in D\big\} </span></p>
  <p>lagrangian, also defines supporting vector: <span class="math inline">(\lambda, \mu, 1)</span> <span
      class="math display">L = (\lambda,\mu,1)^T(u,v,t)</span></p>
  <p>dual function, weak duality <span class="math display">g(\lambda,\mu) = \inf_x L \le L</span></p>
  <p>supporting hyperplane <span class="math inline">S</span> in space of <span class="math inline">\mathcal{G}</span>
    can then be defined:<br />
    <span class="math display">\lambda^Tu = g(\lambda) - t -\mu^T\nu</span></p>
  <p>for convex problems, suppose we have the following 1-D example:</p>
  <p><span class="math display">\begin{aligned}
      \min_x 2x + 1\\
      s.t.\quad 4x^2 - 9 \le 0
      \end{aligned}</span></p>
  <p>hyperplane <span class="math inline">S</span> and <span class="math inline">\mathcal{G}</span> looks like the
    following</p>
  <figure>
    <img src="/assets/img/G.png" style="width:50.0%" alt="" />
    <figcaption>hyperplane of a convex problem, the intercept gives value of <span class="math inline">g(\lambda)</span>
    </figcaption>
  </figure>
  <h3 id="slaters-condition">Slater’s condition</h3>
  <p>Slater’s condition for convex programming states that strong duality holds if there exists an <span
      class="math inline">{\displaystyle x^{*}}</span> s.t. <span class="math inline">{\displaystyle x^{*}}</span> is
    strictly feasible (i.e. all constraints are satisfied and the nonlinear constraints are satisfied with strict
    inequalities).</p>
  <p>Instead of quoting the proof in Boyd’s book, later we using the results in another famous textbook by Ye.</p>
  <h3 id="examples">Examples</h3>
  <p>Here a few examples on convex and non-convex problems.</p>
  <h4 id="linear-programming">Linear programming</h4>
  <p>for the linear programming problems <span class="math display">\min c^Tx, Ax=b, x\ge 0</span> we have: <span
      class="math display">L = c^Tx + \lambda^T(Ax -b) - x^T\nu</span> <span class="math display">g = \begin{cases}
      -b^T\lambda &amp; A^T\lambda +c -\nu = 0 &amp; or &amp;A^T\lambda +c = 0 \\
      -\infty &amp; \text{else}
      \end{cases}</span> which are, also linear programs: <span class="math display">\max b^T\lambda, A^T\lambda +c =
      0</span></p>
  <p>In general, strong duality holds for LP, except primal and dual are both infeasible, which can be found in <span
      class="citation" data-cites="Boyd2004">[<a href="#ref-Boyd2004" role="doc-biblioref">1</a>]</span>.</p>
  <h4 id="minimize-norm-over-a-polyhedron">Minimize norm over a polyhedron</h4>
  <p><span class="math display">\begin{aligned}
      &amp;\min _{x\in\mathbb{R^n}} x^{T} x, A x=b \\
      &amp;L(\lambda, x)= x^{T} x+\lambda^{T}(A x-b) \\
      &amp;\frac{\partial L}{\partial x}=2 x+A^{T} \lambda \\
      &amp;\frac{\partial L}{\partial x \partial x^{T}}=2 I_{n}
      \end{aligned}</span> thus <span class="math inline">x^\star = - A^T\lambda/2</span> <span class="math display">
      g(\lambda) = \frac{-\lambda^T AA^T\lambda}{4} - \lambda^Tb</span> concave since <span
      class="math inline">\displaystyle\mathbf{H} = \frac{-AA^T}{2}</span></p>
  <p><span class="math display">\begin{aligned}
      \partial g &amp;= -b - \frac{1}{2}AA^T\lambda\\
      \lambda^\star &amp;= - 2\cdot(AA^T)^{-1}b \\
      g^\star &amp;= - b^TMb + 2\cdot b^TMb = b^TMb , &amp; M = (AA^T)^{-1} \text{or sudo-inverse} \\
      \end{aligned}
    </span></p>
  <p>We notice the dual problem is unconstrained, i.e., feasible</p>
  <p><span class="math display">\begin{aligned}
      \min_{\lambda} \frac{\lambda^T AA^T\lambda}{4} + \lambda^Tb
      \end{aligned}
    </span></p>
  <h4 id="quadratic-constrained-quadratic-program">Quadratic constrained quadratic program</h4>
  <p><span class="math display">
      \begin{aligned}
      &amp;\min \quad(1 / 2) x^{T} P_{0} x+q_{0}^{T} x+r_{0}\\
      &amp;s.t. \quad(1 / 2) x^{T} P_{i} x+q_{i}^{T} x+r_{i} \leq 0, \quad i=1, \ldots, m
      \end{aligned}</span></p>
  <p>The dual is a quadratic of a function of multiplier <span class="math inline">\lambda</span>.</p>
  <h4 id="entropy">Entropy</h4>
  <p><span class="math display">
      \begin{aligned}
      &amp;\min x^T\log(x)\\
      &amp;s.t.\\
      &amp; Ax\le b \\
      &amp; x^T\mathbf{1} = 1
      \end{aligned}</span></p>
  <p>Then the lagrangian: <span class="math display">\begin{aligned}
      &amp;L = x^T\log(x) + \lambda^TAx - \lambda^Tb + \mu \mathbf{1}^Tx - \mu\\
      &amp;L_x = log(x) + (1 + \mu)\mathbf{1} + A^T\lambda\\
      &amp;x^0 = e^{-(1 + \mu)\mathbf{1} - A^T\lambda} \\
      \Rightarrow \quad&amp; g(\lambda, \mu) = \mathbf{1}^Te^{-(1 + \mu)\mathbf{1} - A^T\lambda},&amp;\qquad \lambda \ge
      0\\
      \text{differentiation on } &amp; \mu\\
      &amp;\mu^0 = log(1^Te^{-A^T\lambda}) -1\\
      g(\lambda) \quad &amp; = b^T\lambda + log(1^Te^{-A^T\lambda}) \\
      &amp;= b^T\lambda + log(\sum_ie^{-a_i^T\lambda}), &amp;\quad \lambda \ge 0
      \end{aligned}
    </span></p>
  <p>This problem is interesting since it is widely used in information theory, statistics (e.g., the logistic
    classifier)</p>
  <h2 class="unnumbered" id="reference">Reference</h2>
  <div id="refs" class="references" role="doc-bibliography">
    <div id="ref-Boyd2004">
      <p>[1] S. Boyd and L. Vandenberghe, <em>Convex Optimization</em>. 2004.</p>
    </div>
  </div>
</body>

</html>